{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 1637108,
          "sourceType": "datasetVersion",
          "datasetId": 967819
        }
      ],
      "dockerImageVersionId": 30301,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isa-ulisboa/greends-pml-2022-2023/blob/main/corn_leaf_disease_detection_with_resnet_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center>Corn Leaf Disease Detection</center>"
      ],
      "metadata": {
        "id": "ALWKnco0BJrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://images.unsplash.com/photo-1503788125311-ae8d4da8ab54?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=774&q=80\"></center>"
      ],
      "metadata": {
        "id": "cK_tPnsdBJrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dependencies"
      ],
      "metadata": {
        "id": "v4gX0JcjBJrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torchvision import models\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:13.692885Z",
          "iopub.execute_input": "2022-12-16T20:35:13.693344Z",
          "iopub.status.idle": "2022-12-16T20:35:13.699948Z",
          "shell.execute_reply.started": "2022-12-16T20:35:13.693301Z",
          "shell.execute_reply": "2022-12-16T20:35:13.698651Z"
        },
        "trusted": true,
        "id": "5zgvQG5_BJrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:13.702684Z",
          "iopub.execute_input": "2022-12-16T20:35:13.703102Z",
          "iopub.status.idle": "2022-12-16T20:35:23.044947Z",
          "shell.execute_reply.started": "2022-12-16T20:35:13.703067Z",
          "shell.execute_reply": "2022-12-16T20:35:23.043746Z"
        },
        "trusted": true,
        "id": "oCOYd5TTBJrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:23.046781Z",
          "iopub.execute_input": "2022-12-16T20:35:23.047446Z",
          "iopub.status.idle": "2022-12-16T20:35:23.052661Z",
          "shell.execute_reply.started": "2022-12-16T20:35:23.047405Z",
          "shell.execute_reply": "2022-12-16T20:35:23.051656Z"
        },
        "trusted": true,
        "id": "8d0e96u9BJrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing the Device"
      ],
      "metadata": {
        "id": "WgaHJMbKBJrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:23.05394Z",
          "iopub.execute_input": "2022-12-16T20:35:23.054266Z",
          "iopub.status.idle": "2022-12-16T20:35:23.066542Z",
          "shell.execute_reply.started": "2022-12-16T20:35:23.05423Z",
          "shell.execute_reply": "2022-12-16T20:35:23.06557Z"
        },
        "trusted": true,
        "id": "4gAtKdNnBJrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the Data Into Train, Validation and Test Sets"
      ],
      "metadata": {
        "id": "jYCfM-6YBJrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the data is not seperated, it should be splitted into Train, Validation and Test sets. To do that, I used splitfolders library and splitted the data into Train, Validation and Test sets by 70%, 20% and 10% respectively."
      ],
      "metadata": {
        "id": "beoLU0CWBJrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitfolders.ratio(\"/kaggle/input/corn-or-maize-leaf-disease-dataset/data\",\n",
        "            output=\"splitted_data\",\n",
        "            seed=42,\n",
        "            ratio=(.7, .2, .1),\n",
        "            group_prefix=None,\n",
        "            move=False)"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:23.071047Z",
          "iopub.execute_input": "2022-12-16T20:35:23.071763Z",
          "iopub.status.idle": "2022-12-16T20:35:31.454718Z",
          "shell.execute_reply.started": "2022-12-16T20:35:23.071735Z",
          "shell.execute_reply": "2022-12-16T20:35:31.453813Z"
        },
        "trusted": true,
        "id": "anIqzc7vBJrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Transformation"
      ],
      "metadata": {
        "id": "VrE6M3yWBJrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resizing and Normalizing methods are applied to all of the datasets but data augmentation techniques are only applied to Train set in order to richen the data so that the model possibly will yield more accurate results."
      ],
      "metadata": {
        "id": "geTQf0HOBJrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.GaussianBlur(kernel_size=(3, 7), sigma=(0.1, 2)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# -------- Transformation for the Whole Dataset for Visualization Purposes --------\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:31.457505Z",
          "iopub.execute_input": "2022-12-16T20:35:31.458426Z",
          "iopub.status.idle": "2022-12-16T20:35:31.467628Z",
          "shell.execute_reply.started": "2022-12-16T20:35:31.458386Z",
          "shell.execute_reply": "2022-12-16T20:35:31.466668Z"
        },
        "trusted": true,
        "id": "8CtCHvmJBJrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Data"
      ],
      "metadata": {
        "id": "xRt_d0PfBJrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = datasets.ImageFolder(root=\"/kaggle/working/splitted_data/train\",\n",
        "                              transform=train_transform)\n",
        "\n",
        "val = datasets.ImageFolder(root=\"/kaggle/working/splitted_data/val\",\n",
        "                              transform=val_transform)\n",
        "\n",
        "test = datasets.ImageFolder(root=\"/kaggle/working/splitted_data/test\",\n",
        "                              transform=test_transform)\n",
        "\n",
        "# -------- Getting the Whole Dataset for Visualization Purposes --------\n",
        "data = datasets.ImageFolder(root=\"/kaggle/input/corn-or-maize-leaf-disease-dataset/data\",\n",
        "                              transform=data_transform)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:31.469305Z",
          "iopub.execute_input": "2022-12-16T20:35:31.469691Z",
          "iopub.status.idle": "2022-12-16T20:35:31.525269Z",
          "shell.execute_reply.started": "2022-12-16T20:35:31.469654Z",
          "shell.execute_reply": "2022-12-16T20:35:31.524399Z"
        },
        "trusted": true,
        "id": "EgKD5uaKBJrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset Labels:\\n\", train.class_to_idx, \"\\n\")\n",
        "\n",
        "for name, dataset in zip([\"TRAIN\", \"VALIDATION\", \"TEST\"], [train, val, test]):\n",
        "    images_per_class = pd.Series(dataset.targets).value_counts()\n",
        "    print(f\"Images per Class in {name}:\")\n",
        "    print(images_per_class, \"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:31.526547Z",
          "iopub.execute_input": "2022-12-16T20:35:31.527188Z",
          "iopub.status.idle": "2022-12-16T20:35:31.539301Z",
          "shell.execute_reply.started": "2022-12-16T20:35:31.527151Z",
          "shell.execute_reply": "2022-12-16T20:35:31.538311Z"
        },
        "trusted": true,
        "id": "UzbClN5gBJrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting a few Images"
      ],
      "metadata": {
        "id": "M5ElfMvqBJrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Images plotted below did not undergo a data augmentation process. Therefore, they seem to be normal."
      ],
      "metadata": {
        "id": "hfOPbfIWBJrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_for_viz = {v: k for k, v in data.class_to_idx.items()}\n",
        "\n",
        "fig, ax = plt.subplots(3, 5, figsize=(15, 10))\n",
        "ax = ax.flatten()\n",
        "for i in range(15):\n",
        "    sample = random.randint(0, len(data))\n",
        "    ax[i].imshow(data[sample][0].permute(1, 2, 0))\n",
        "    ax[i].title.set_text(labels_for_viz[data[sample][1]])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:31.540592Z",
          "iopub.execute_input": "2022-12-16T20:35:31.541732Z",
          "iopub.status.idle": "2022-12-16T20:35:33.435707Z",
          "shell.execute_reply.started": "2022-12-16T20:35:31.541696Z",
          "shell.execute_reply": "2022-12-16T20:35:33.434797Z"
        },
        "trusted": true,
        "id": "efXXRS5WBJrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the Augmented Images"
      ],
      "metadata": {
        "id": "xtgxYtrKBJrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because data augmentation methods are applied to Train set, it can be seen that images are blurred, horizontally and vertically flipped compared to the unaugmented images."
      ],
      "metadata": {
        "id": "LGDdrFhkBJrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(3, 5, figsize=(15, 10))\n",
        "ax = ax.flatten()\n",
        "for i in range(15):\n",
        "    sample = random.randint(0, len(train))\n",
        "    ax[i].imshow(train[sample][0].permute(1, 2, 0))\n",
        "    ax[i].title.set_text(labels_for_viz[train[sample][1]])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:33.437098Z",
          "iopub.execute_input": "2022-12-16T20:35:33.437687Z",
          "iopub.status.idle": "2022-12-16T20:35:35.807274Z",
          "shell.execute_reply.started": "2022-12-16T20:35:33.437648Z",
          "shell.execute_reply": "2022-12-16T20:35:35.806302Z"
        },
        "trusted": true,
        "id": "JDxFL9Q0BJrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before passing the data in a neural network, it should be batched and shuffled. Since test will not be used in training process, there is no need to shuffle it."
      ],
      "metadata": {
        "id": "XJtW61wRBJrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(dataset=train,\n",
        "                             batch_size=32,\n",
        "                             num_workers=2,\n",
        "                             shuffle=True)\n",
        "\n",
        "val_dataloader = DataLoader(dataset=val,\n",
        "                             batch_size=32,\n",
        "                             num_workers=2,\n",
        "                             shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test,\n",
        "                             batch_size=32,\n",
        "                             num_workers=2,\n",
        "                             shuffle=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:35.808737Z",
          "iopub.execute_input": "2022-12-16T20:35:35.809491Z",
          "iopub.status.idle": "2022-12-16T20:35:35.817027Z",
          "shell.execute_reply.started": "2022-12-16T20:35:35.80944Z",
          "shell.execute_reply": "2022-12-16T20:35:35.815883Z"
        },
        "trusted": true,
        "id": "GpN4OEL-BJrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When looking at the order of the data in relevant folder, it can be observed that images from class \"Blight\" comes first, \"Common_Rust\" second, \"Gray_Leaf_Spot\" third and \"Healthy\" last. An order as such can be problematic for the model to train unbiasedly. Let's take a batch and look at the labels to make sure that the data is shuffled."
      ],
      "metadata": {
        "id": "yhz4FMxABJrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = next(iter(train_dataloader))\n",
        "print(\"Batch and Image Shape:\", img.shape, \"--> [batch_size, color_channels, height, width]\")\n",
        "print(\"\\nLabels:\", label)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:35.818643Z",
          "iopub.execute_input": "2022-12-16T20:35:35.818968Z",
          "iopub.status.idle": "2022-12-16T20:35:37.284332Z",
          "shell.execute_reply.started": "2022-12-16T20:35:35.818936Z",
          "shell.execute_reply": "2022-12-16T20:35:37.283223Z"
        },
        "trusted": true,
        "id": "wBl8YwnQBJrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Deep Learning Model"
      ],
      "metadata": {
        "id": "FxZuVWQcBJrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom neural network architecture can be designed for this machine learning problem. With a neural network similar to the one below, an accuracy score around 80-84% is obtained. But for this project, I opt to apply Transfer Learning using ResNet18 architecture."
      ],
      "metadata": {
        "id": "g9dqLYwrBJrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=131072, out_features=32), # in_features are selected based on the output that Flatten layer yields\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=32, out_features=16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=16, out_features=4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:37.287944Z",
          "iopub.execute_input": "2022-12-16T20:35:37.288266Z",
          "iopub.status.idle": "2022-12-16T20:35:37.3Z",
          "shell.execute_reply.started": "2022-12-16T20:35:37.288235Z",
          "shell.execute_reply": "2022-12-16T20:35:37.299078Z"
        },
        "trusted": true,
        "id": "FReXGSr5BJrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning - ResNet18"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-13T19:43:26.470758Z",
          "iopub.execute_input": "2022-12-13T19:43:26.471145Z",
          "iopub.status.idle": "2022-12-13T19:43:26.475492Z",
          "shell.execute_reply.started": "2022-12-13T19:43:26.471113Z",
          "shell.execute_reply": "2022-12-13T19:43:26.474475Z"
        },
        "id": "sEdD8w_RBJrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning is a process of utilizing pre-trained machine learning models and applying them to a related task without need of models' learning from scratch for saving time and possibly better performance."
      ],
      "metadata": {
        "id": "ViSjjIvbBJrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(in_features=512, out_features=4)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:37.305526Z",
          "iopub.execute_input": "2022-12-16T20:35:37.305947Z",
          "iopub.status.idle": "2022-12-16T20:35:37.567239Z",
          "shell.execute_reply.started": "2022-12-16T20:35:37.305917Z",
          "shell.execute_reply": "2022-12-16T20:35:37.566176Z"
        },
        "trusted": true,
        "id": "8FfLPuzEBJrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:37.568942Z",
          "iopub.execute_input": "2022-12-16T20:35:37.569315Z",
          "iopub.status.idle": "2022-12-16T20:35:37.576388Z",
          "shell.execute_reply.started": "2022-12-16T20:35:37.569278Z",
          "shell.execute_reply": "2022-12-16T20:35:37.575277Z"
        },
        "trusted": true,
        "id": "NvkLr5W2BJrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Loss Function and Choosing an Optimizer"
      ],
      "metadata": {
        "id": "eHDOvoY-BJrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=10e-3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:37.577755Z",
          "iopub.execute_input": "2022-12-16T20:35:37.578807Z",
          "iopub.status.idle": "2022-12-16T20:35:37.588268Z",
          "shell.execute_reply.started": "2022-12-16T20:35:37.578768Z",
          "shell.execute_reply": "2022-12-16T20:35:37.587281Z"
        },
        "trusted": true,
        "id": "o7eXAoVjBJrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Train and Test Functions"
      ],
      "metadata": {
        "id": "mAd7xU6RBJrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate_model(num_epochs):\n",
        "    val_best_accuracy = 0.0\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    train_accuracy_history = []\n",
        "    val_accuracy_history = []\n",
        "\n",
        "    print(\"Training begins...\")\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        running_train_loss = 0.0\n",
        "        running_train_accuracy = 0.0\n",
        "        train_total = 0\n",
        "        running_val_accuracy = 0.0\n",
        "        running_val_loss = 0.0\n",
        "        val_total = 0\n",
        "\n",
        "\n",
        "        # TRAINING LOOP\n",
        "        for data in train_dataloader:\n",
        "            inputs, outputs = data\n",
        "            inputs, outputs = inputs.to(device), outputs.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(inputs)\n",
        "            train_loss = loss_fn(predictions, outputs)\n",
        "            _, train_predicted = torch.max(predictions, 1)\n",
        "            running_train_accuracy += (train_predicted == outputs).sum().item()\n",
        "            train_total += outputs.size(0)\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train_loss += train_loss.item()\n",
        "\n",
        "        train_loss_value = running_train_loss/len(train_dataloader)\n",
        "        train_loss_history.append(train_loss_value)\n",
        "        train_accuracy = (100*running_train_accuracy)/train_total\n",
        "        train_accuracy_history.append(train_accuracy)\n",
        "\n",
        "        # VALIDATION LOOP\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for data in val_dataloader:\n",
        "                inputs, outputs = data\n",
        "                inputs, outputs = inputs.to(device), outputs.to(device)\n",
        "                predictions = model(inputs)\n",
        "                val_loss = loss_fn(predictions, outputs)\n",
        "\n",
        "                _, val_predicted = torch.max(predictions, 1)\n",
        "                running_val_loss += val_loss.item()\n",
        "                val_total += outputs.size(0)\n",
        "                running_val_accuracy += (val_predicted == outputs).sum().item()\n",
        "\n",
        "        val_loss_value = running_val_loss/len(val_dataloader)\n",
        "        val_loss_history.append(val_loss_value)\n",
        "        val_accuracy = (100*running_val_accuracy)/val_total\n",
        "        val_accuracy_history.append(val_accuracy)\n",
        "\n",
        "        if val_accuracy > val_best_accuracy:\n",
        "            torch.save(model.state_dict(), \"model.pth\")\n",
        "            val_best_accuracy = val_accuracy\n",
        "\n",
        "        print(\"Completed Epoch: \", epoch, \"- Training Accuracy: %d\" %train_accuracy, \"- Validation Accuracy: %d\" %val_accuracy, \"- Training Loss: %.4f\" %train_loss_value, \"- Validation Loss: %.4f\" %val_loss_value)\n",
        "    return train_accuracy_history, val_accuracy_history, train_loss_history, val_loss_history"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:37.591375Z",
          "iopub.execute_input": "2022-12-16T20:35:37.592287Z",
          "iopub.status.idle": "2022-12-16T20:35:37.605968Z",
          "shell.execute_reply.started": "2022-12-16T20:35:37.592247Z",
          "shell.execute_reply": "2022-12-16T20:35:37.604764Z"
        },
        "trusted": true,
        "id": "TFG-DcA8BJrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model():\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    model.fc = nn.Linear(in_features=512, out_features=4)\n",
        "    model = model.to(device)\n",
        "    path = \"model.pth\"\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    model.eval()\n",
        "\n",
        "    running_accuracy = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_dataloader:\n",
        "            inputs, outputs = data\n",
        "            inputs, outputs = inputs.to(device), outputs.to(device)\n",
        "            outputs = outputs.to(torch.float32)\n",
        "            predictions = model(inputs)\n",
        "            _, predicted = torch.max(predictions, 1)\n",
        "            total += outputs.size(0)\n",
        "            running_accuracy += (predicted == outputs).sum().item()\n",
        "            accuracy = 100*running_accuracy/total\n",
        "\n",
        "            all_predictions.append(list(predicted.to(\"cpu\").numpy()))\n",
        "        print(\"Test Accuracy: \", accuracy)\n",
        "    return np.hstack(all_predictions) # returns a flattened array of batches of predictions"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:37.608553Z",
          "iopub.execute_input": "2022-12-16T20:35:37.608988Z",
          "iopub.status.idle": "2022-12-16T20:35:37.626837Z",
          "shell.execute_reply.started": "2022-12-16T20:35:37.608947Z",
          "shell.execute_reply": "2022-12-16T20:35:37.625898Z"
        },
        "trusted": true,
        "id": "K-BCguacBJrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 15\n",
        "train_accuracy_history, val_accuracy_history, train_loss_history, val_loss_history = train_and_validate_model(EPOCHS)\n",
        "print(\"Training finished...\\n\")\n",
        "all_preds = test_model()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:35:37.62817Z",
          "iopub.execute_input": "2022-12-16T20:35:37.628746Z",
          "iopub.status.idle": "2022-12-16T20:45:10.168104Z",
          "shell.execute_reply.started": "2022-12-16T20:35:37.628711Z",
          "shell.execute_reply": "2022-12-16T20:45:10.162027Z"
        },
        "trusted": true,
        "id": "OuzG_6zFBJrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(12, 10))\n",
        "ax = ax.flatten()\n",
        "\n",
        "ax[0].plot(train_loss_history, label=\"Train\")\n",
        "ax[0].plot(val_loss_history, label=\"Validation\")\n",
        "ax[0].title.set_text(\"Loss\")\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(train_accuracy_history, label=\"Train\")\n",
        "ax[1].plot(val_accuracy_history, label=\"Validation\")\n",
        "ax[1].title.set_text(\"Accuracy Score\")\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:45:10.174017Z",
          "iopub.execute_input": "2022-12-16T20:45:10.176896Z",
          "iopub.status.idle": "2022-12-16T20:45:10.951949Z",
          "shell.execute_reply.started": "2022-12-16T20:45:10.176854Z",
          "shell.execute_reply": "2022-12-16T20:45:10.950962Z"
        },
        "trusted": true,
        "id": "-3AOhBc3BJrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test.targets, all_preds))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:45:10.95617Z",
          "iopub.execute_input": "2022-12-16T20:45:10.95833Z",
          "iopub.status.idle": "2022-12-16T20:45:10.974772Z",
          "shell.execute_reply.started": "2022-12-16T20:45:10.958292Z",
          "shell.execute_reply": "2022-12-16T20:45:10.973889Z"
        },
        "trusted": true,
        "id": "tMkiwzNaBJrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(confusion_matrix(test.targets, all_preds), cmap=\"Blues\", annot=True, fmt=\"d\")\n",
        "plt.title(\"Confusion Matrix\", size=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:45:10.978656Z",
          "iopub.execute_input": "2022-12-16T20:45:10.98113Z",
          "iopub.status.idle": "2022-12-16T20:45:11.268457Z",
          "shell.execute_reply.started": "2022-12-16T20:45:10.981093Z",
          "shell.execute_reply": "2022-12-16T20:45:11.267558Z"
        },
        "trusted": true,
        "id": "BR5cd5fTBJrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual Testing"
      ],
      "metadata": {
        "id": "Cv2WB2GABJrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is always a good practice to test the model manually and try to understand what is going on in prediction. Let's choose a single sample out of test set and make the model predict that single image."
      ],
      "metadata": {
        "id": "Qt9wXePxBJrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
        "ax = ax.flatten()\n",
        "\n",
        "ax[0].imshow(data[-1][0].permute(1, 2, 0))\n",
        "ax[0].title.set_text(f\"{labels_for_viz[data[-1][1]]} (Before Preprocessing)\")\n",
        "\n",
        "ax[1].imshow(test[-1][0].permute(1, 2, 0))\n",
        "ax[1].title.set_text(f\"{labels_for_viz[test[-1][1]]} (After Preprocessing)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:45:11.271664Z",
          "iopub.execute_input": "2022-12-16T20:45:11.271948Z",
          "iopub.status.idle": "2022-12-16T20:45:11.647539Z",
          "shell.execute_reply.started": "2022-12-16T20:45:11.271922Z",
          "shell.execute_reply": "2022-12-16T20:45:11.646622Z"
        },
        "trusted": true,
        "id": "3O6y26kYBJrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code above, I fetched the last sample of test set and its class is \"Healthy\". Now, it is time to find out what the probabilities are for each class."
      ],
      "metadata": {
        "id": "S234g5t6BJrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single_prediction = model.forward((test[-1][0]).to(device).unsqueeze(0))\n",
        "single_prediction"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:45:11.649109Z",
          "iopub.execute_input": "2022-12-16T20:45:11.650175Z",
          "iopub.status.idle": "2022-12-16T20:45:11.675077Z",
          "shell.execute_reply.started": "2022-12-16T20:45:11.650135Z",
          "shell.execute_reply": "2022-12-16T20:45:11.674079Z"
        },
        "trusted": true,
        "id": "vR2gB83eBJrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then index of the class with highest probability should be taken for printing the corresponding label."
      ],
      "metadata": {
        "id": "jWfYGq-ABJrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single_prediction = int(torch.max(single_prediction, 1)[1])\n",
        "print(\"Predicted Class:\", labels_for_viz[single_prediction])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-16T20:45:11.677445Z",
          "iopub.execute_input": "2022-12-16T20:45:11.678381Z",
          "iopub.status.idle": "2022-12-16T20:45:11.686539Z",
          "shell.execute_reply.started": "2022-12-16T20:45:11.678337Z",
          "shell.execute_reply": "2022-12-16T20:45:11.685456Z"
        },
        "trusted": true,
        "id": "BAa_XRgCBJrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center>Thank you for reading the notebook. Hope it helps. Preparing notebooks takes a great deal of time. If you liked the notebook, please do not forget to give an upvote. Peace out ✌️</center>"
      ],
      "metadata": {
        "id": "KPtrHOavBJrT"
      }
    }
  ]
}